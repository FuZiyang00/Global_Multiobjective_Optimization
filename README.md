# Global and Multiobjective Optimization project: Feature selection with Genetic Algorithms: Filter approach vs Wrapper approach

# Introduction

This project explores feature selection using Genetic Algorithms (GAs). 

**Feature selection** is a critical step in machine learning and data preprocessing, aiming to improve model performance, reduce overfitting, and lower computational costs by selecting the most relevant features.

The project investigates and compares two fundamental approaches for feature selection:

1. **Filter Approach**: a model-independent method that selects features based on statistical measures such as correlation or mutual information.

2. **Wrapper Approach**: a model-dependent method that evaluates feature subsets by training and testing a specific machine learning model.

# Dataset  

The dataset used in this project contains records of companies that have purchased credit insurance over their invoices. Each record corresponds to **a single insurance contract** and includes both input features and a binary target label.

### Target Variable

- **Label**: a **binary indicator (0 or 1)** representing whether an issued contract has been profitable (1) or unprofitable (0) to the insurer.

This variable serves as the classification target for the feature selection and optimization tasks.

### Features

The dataset comprises 126 input features that capture a comprehensive profile of each company. These features fall into three main categories:

1. **Financial Statements Data**: quantitative indicators derived from companies' balance sheets and income statements (e.g., revenue, debt ratios, liquidity).

2. **Anagraphic Data**: attributes of the companies, including: number of employees, geographical location and industry sector 

3. **Insurerâ€™s Preliminary Evaluation Metrics**: internal risk assessment metrics generated by the insurer prior to issuing the credit insurance policy. These may include credit scores and risk flags

This diverse set of features allows for a rich and realistic evaluation of feature selection methods, especially under the complexity of real-world, high-dimensional data.

# Filter method

### Source: 
https://www.researchgate.net/publication/3693063_Fast_feature_selection_with_genetic_algorithms_A_filter_approach 

### Fitness criterion: Inconsistency rate 

The fitness of each individual is measured by its **inconsistency rate**.

The inconsistency rate specifies to what extent the reduced data still represent the original dataset and can be considered a measure of how much inconsistent the data become when only a subset of attributes is considered. 

Consider the following table: 

| Observation  | attribute 1 | attribute 2 | attribute 3 | attribute 4| class | 
|----------    |----------   | ----------  | ----------  |----------  |-------|
| items_i      |   0          |   1          | 2           |     4    |0|
| items_j      |   0          |   1          | 3           |     4    | 1|

The two items have different class values but differs only in attribute 3 so that if the feature subset {att_1, att_2, att_4} is considered, we get an inconsistency in the data. 


| Observation  | attribute 1 | attribute 2 | attribute 4| class | 
|----------    |----------   | ----------  |----------  |-------|
| items_i      |   0          |   1          |      4    |0      |
| items_j      |   0          |   1          |      4    | 1     |

The table above has only a subset of the original features, and the two items are equal with respect to the attribute values but differ for the class attribute: **for a learning algorithm there is an example that has been classified with two different label: this is inconsistent** 

Inconsistency is introduced in the data when the number
of attributes is reduced; the rate measures how much inconsistency is introduced when only a certain feature subset is considered. 

The rate is computed as follows: 
1. group all the records that are equal based on the selected features; 
2. in each group: 
    - if all the class labels are the same $ \rightarrow $ no inconsistency
    - if there are different class labels $ \rightarrow $ some inconsitency 
    - cont the number of inconsistent records as 
    $$ \text{Group size} - \text{number of records with the majority class}
3. add up all these inconsistencies across all the groups; 
4. divide this sum by the total number of records to get the **inconsistency rate** 

This criterion tends to be non informative when datasets contain continuous attributes with many values, so the continous data have been discretize into 10 buckets based on the feature distribution. 

## Wrapper method 

### Fitness criterion: Precision 

The fitness of each individual is measured by the **precision score** achieved by a **XGBoost** classification model. 

$$ \frac{\text{True positives}} {\text{True positives + False positives}} $$ 

Precision comes particularly handy when false positives (so not profitable insurance contracts) are costly. 


# Genetic algorithm 

The core of this project relies on a Genetic Algorithm (GA) to perform feature selection, confronting the performances of a filter-based and a wrapper-based method. 

### Chromosome Genotype 

Each individual in the population is represented as a binary vector of length 126, where each bit corresponds to a feature:
- 1 = feature is selected
- 0 = feature is excluded

### Operators 

1. **Initialization**: the initial population consists of randomly generated binary vectors;

2. **Selection**: Tournament Selection is used to select parents;

3. **Crossover**: Uniform Crossover is applied to parent pairs;

4. **Mutation**: Bit Flip Mutation is used to introduce small random changes.


### Fitness 

The fitness of each individual depends on the method used: 

- **Filter method**: the algorithm tries to **minimize** the following fitnes function:

$$\text{Inconsistency rate} + \text{Penalty} \times (\frac{\text{Number of 1s}}{Total number of features})$$

This objective balances data consistency with model simplicity by discouraging overly large feature subsets. 

- **Wrapper method**: the algorithm tries to **maximize** the following fitnes function: 

$$
\text{Precision} - \text{Penalty} \times (\frac{\text{Number of 1s}}{Total number of features})
$$
This objective promotes feature subsets that yield high classification performance while penalizing overly complex models

# Benchmarks

|                  | Precision    | Recall     | Accuracy   | AUC   | 
|----------        |----------    |----------  |----------  |-------|
| Baseline model   | 0.8156       |   0.8400   |  0.7284    |0.6560 |
| FIlter GA model  | 0.7986       |   0.8664   |  0.7266    |0.6135 |
| Wrapper GA model | 0.8176       |   0.8311   |  0.7249    |0.6533 |

> [!WARNING]  
> Install the dependencies listed in requirements.txt before running the experiment
